Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Policy/Curiosity Value Estimate,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Losses/Curiosity Forward Loss,Losses/Curiosity Inverse Loss,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Policy/Curiosity Reward,Is Training
10000,1.5823892,-0.23609982,0.21207666,0.03624256,0.101343326,0.0002996986,0.19989952,0.009989962,0.10596279,1.5759871,499.0,-1.8724999008700252,-1.8724999008700252,6.690695032477379,1.0
20000,1.4965641,-0.30194953,0.8452288,0.011347892,0.10173835,0.00029915827,0.19971943,0.009971971,0.08766551,1.3399442,499.0,-1.4126086746059034,-1.4126086746059034,9.62213673280633,1.0
30000,1.3932252,-0.2783022,1.4498475,0.008012989,0.102629684,0.00029855888,0.1995196,0.009952011,0.10402745,1.032649,499.0,-0.949411815361065,-0.949411815361065,11.42124330997467,1.0
40000,1.2942765,-0.24779737,1.7554924,0.0073813302,0.09530228,0.00029795826,0.1993194,0.0099320095,0.10586379,0.9393198,499.0,-1.0447826204902453,-1.0447826204902453,11.431260793105416,1.0
